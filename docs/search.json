[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reddit Analysis",
    "section": "",
    "text": "Reddit is a highly used pseudo-anonymous social media website.\nI’ve pulled archives of Reddit posts and comments from pushshift, about one terabyte of compressed comment and post data. That’s quite a bit of data to be sorting through!\n\n\n\nReading through the Reddit data is a bit of a problem because of the size of the data. I’m using Apache spark to efficiently filter the data. The R interface to spark I’m using is sparklyr, which also includes machine learning capabilities.\n\n\n\nSentiment analyses are the bread and butter of finding out the general vibe of an online space. Obviously there are many ways to describe the sentiments of the communities we’re going to be analyzing but this gives us a good way to get off the ground.\n\n\n\nThe actual things discussed in a subreddit (community of practice) can vary pretty wildly from what just the name of the subreddit would imply. For spaces related to programming or education, having a list of these gives us an idea of what broad communites think is needed to discuss to learn programming.\n\n\n\nThis is probably the most exciting method of the bunch. Quantitative ethnographies use an epistemic network analysis to address the underlying values of a group through the help of a semi automated qualitative coding process."
  },
  {
    "objectID": "source-data.html",
    "href": "source-data.html",
    "title": "1  Source Reddit Data",
    "section": "",
    "text": "I’ve pulled quite a bit of Reddit data to process. Files that begin with RS are subreddit data including post titles, selftext, user, etc. Files that begin with RC are comment data.\nLets take a look at how big some of this stuff is:\n\n\nCode\nprint(\"Years of posts data: \")\n\n\n[1] \"Years of posts data: \"\n\n\nCode\nfs::dir_ls(\"/scratch1/tsranso/reddit/posts/\")\n\n\n/scratch1/tsranso/reddit/posts/2014 /scratch1/tsranso/reddit/posts/2015 \n/scratch1/tsranso/reddit/posts/2016 /scratch1/tsranso/reddit/posts/2017 \n\n\nCode\nprint(\"Size of one month of data: \")\n\n\n[1] \"Size of one month of data: \"\n\n\nCode\nfs::file_size(\"/scratch1/tsranso/reddit/posts/2017/RS_2017-01.json\")\n\n\n16.1G"
  },
  {
    "objectID": "source-data.html#spark",
    "href": "source-data.html#spark",
    "title": "1  Source Reddit Data",
    "section": "1.2 Spark",
    "text": "1.2 Spark\nI’m using the Palmetto cluster to make quick work of processing through our Reddit data.\n\n1.2.1 Set up and connect to spark\n\n\nCode\nSys.setenv(SPARK_HOME = \"/software/external/spark/3.3.1\")\nSys.setenv(JAVA_HOME = \"/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/openjdk-11.0.15_10-xo4fjahmlsjch52sftpoxby6kwbdfoib\")\nlibrary(sparklyr)\n\noptions(sparklyr.log.console = TRUE)\nconfig <- spark_config()\nconfig[\"sparklyr.shell.driver-memory\"] <- \"10g\"\nconfig[\"spark.executor.memory\"] <- \"15G\"  # typically 4g\n\nspark_master_info <- Sys.getenv(\"RSESSION_LOG_FILE\") %>%\n    dirname %>%\n    paste0(\"/spark_master.info\") %>%\n    readLines(n = 1)\n\n# master node needs to be modified with each new job\nsc <- spark_connect(master = spark_master_info, config = config)\n\n\n\n\n1.2.2 Data filtering functions\nWe need a few functions to read through the Reddit data, for example here’s something we can use to get out the text posts for a given subreddit.\n\n\nCode\npull_posts_of_subreddit <- function(years = c(2014, 2015, 2016, 2017), sr = \"Python\",\n    write_to_dir = FALSE) {\n    for (year in years) {\n        year_dir <- paste0(\"/scratch1/tsranso/reddit/posts/\", year)\n        message(\"Reading from directory: \", year_dir)\n        message(\"file exists: \", file.exists(year_dir))\n        posts <- spark_read_json(sc, path = year_dir, memory = FALSE) %>%\n            filter(subreddit %in% c(sr)) %>%\n            mutate(selftext = regexp_replace(selftext, \"\\\\n|&nbsp;|<[^>]*>|[^A-Za-z|']\",\n                \" \")) %>%\n            mutate(selftext = str_trim(selftext)) %>%\n            filter(!selftext %in% c(\"\", \"deleted\", \"title\", \"removed\"))\n\n        if (write_to_dir) {\n            output_directory <- paste0(\"/scratch1/tsranso/reddit/subreddit-posts/\",\n                sr, \"/\", year)\n            message(\"Writing filtered post data to: \", output_directory)\n            spark_write_json(posts, path = output_directory, mode = \"overwrite\")\n        }\n    }\n\n    return(posts)\n}\npull_posts_of_subreddit(write_to_dir = TRUE)\n\n\nAnd another one to get all the comments from a subreddit:\n\n\nCode\npull_comments_of_subreddit <- function(years = c(2014, 2014, 2016, 2017), sr = \"python\",\n    write_to_dir = FALSE) {\n    for (year in years) {\n        year_dir <- paste0(\"/scratch1/tsranso/reddit/comments/\", year)\n        comments <- spark_read_json(sc, path = year_dir, memory = FALSE) %>%\n            filter(subreddit %in% c(sr) & selftext != \"\") %>%\n            mutate(selftext = regexp_replace(selftext, \"\\\\n|&nbsp;|<[^>]*>|[^A-Za-z|']\",\n                \" \")) %>%\n            mutate(selftext = str_trim(selftext)) %>%\n            filter(!selftext %in% c(\"\", \"deleted\", \"title\", \"removed\"))\n\n        if (write_to_dir) {\n            output_directory <- paste0(\"/scratch1/tsranso/reddit/subreddit-comments/\",\n                sr, \"/\", year)\n            message(\"Writing filtered comment data to: \", output_directory)\n            spark_write_json(comments, path = output_directory, mode = \"overwrite\")\n        }\n    }\n\n    return(posts)\n}"
  },
  {
    "objectID": "source-data.html#subreddits-of-interest",
    "href": "source-data.html#subreddits-of-interest",
    "title": "1  Source Reddit Data",
    "section": "1.3 Subreddits of interest",
    "text": "1.3 Subreddits of interest\nThere are lots of great (and many not so great) communities on Reddit. Here is the list of subreddits that are considered in this project:\n\n\nCode\nsubs_of_interest <- c(\"learnpython\", \"Python\", \"learnprogramming\", \"programming\",\n    \"AskProgramming\", \"CSEducation\", \"computerscience\", \"AskComputerScience\", \"compsci\",\n    \"AskCompSci\", \"cscareerquestions\", \"coding\", \"javascript\", \"learnjavascript\",\n    \"java\", \"learnjava\", \"haskell\", \"haskellquestions\", \"C_Programming\")\n# knitr::kable(subs_of_interest, caption = 'Subreddits of interest')\nsubs_of_interest %>%\n    kable() %>%\n    kable_styling(\"striped\") %>%\n    scroll_box(height = \"200px\")\n\n\n\n\n \n  \n    x \n  \n \n\n  \n    learnpython \n  \n  \n    Python \n  \n  \n    learnprogramming \n  \n  \n    programming \n  \n  \n    AskProgramming \n  \n  \n    CSEducation \n  \n  \n    computerscience \n  \n  \n    AskComputerScience \n  \n  \n    compsci \n  \n  \n    AskCompSci \n  \n  \n    cscareerquestions \n  \n  \n    coding \n  \n  \n    javascript \n  \n  \n    learnjavascript \n  \n  \n    java \n  \n  \n    learnjava \n  \n  \n    haskell \n  \n  \n    haskellquestions \n  \n  \n    C_Programming \n  \n\n\n\n\n\n\nPull PostsPull Comments\n\n\nLet’s loop through the subreddits of interest and pull out the text posts! Note the write_to_dir option here that caches the pulled posts into smaller files so we don’t have to do this repeatedly.\n\n\nCode\nfor (sub in subs_of_interest) {\n    pull_posts_of_subreddit(sr = sub, write_to_dir = TRUE)\n}\n\n\n\n\n\n\nCode\nfor (sub in subs_of_interest) {\n    pull_comments_of_subreddit(sr = sub, write_to_dir = TRUE)\n}"
  },
  {
    "objectID": "source-data.html#post-data",
    "href": "source-data.html#post-data",
    "title": "1  Source Reddit Data",
    "section": "2.1 Post Data",
    "text": "2.1 Post Data\n\n\nCode\ninput_directory <- paste0(\"/scratch1/tsranso/reddit/subreddit-posts/\", sr, \"/\", year)\nposts <- spark_read_json(sc, path = input_directory, memory = FALSE)\nposts %>%\n    head %>%\n    print"
  },
  {
    "objectID": "source-data.html#comment-data",
    "href": "source-data.html#comment-data",
    "title": "1  Source Reddit Data",
    "section": "2.2 Comment Data",
    "text": "2.2 Comment Data\n\n\nCode\ninput_directory <- paste0(\"/scratch1/tsranso/reddit/subreddit-comments/\", sr, \"/\",\n    year)\ncomments <- spark_read_json(sc, path = input_directory, memory = FALSE)\ncomments %>%\n    head %>%\n    print"
  },
  {
    "objectID": "sentiment-analysis.html",
    "href": "sentiment-analysis.html",
    "title": "2  Sentiment Analysis",
    "section": "",
    "text": "Conducting the analysisPlotting the results\n\n\n\n\nCode\nconduct_sentiment_analysis <- function(sr = \"Python\", year = 2014){\n  input_directory <- paste0(\"/scratch1/tsranso/reddit/subreddit-posts/\", sr, \"/\", year)\n  \n  message(\"Opening input directory: \", input_directory)\n  posts <- spark_read_json(sc, path = input_directory, memory = FALSE)\n  message(\"Running analysis...\")\n  # sentiments <- posts %>% \n  #   collect() %>% \n  #   dplyr::mutate(dialogue_split = sentimentr::get_sentences(selftext), sub = sr) %$%\n  #   sentiment_by(dialogue_split, sub)\n  \nsentiments <- posts %>% \n    collect() %>% \n    sentimentr::get_sentences() %$%\n    sentiment_by(selftext)\n  \n  message(\"Computed sentiments for subreddit: \", sr, \" during \", year)\n  return(sentiments)\n}\n\n\n\n\n\n\nCode\nplot_sentiments <- function(sentiments, sr = \"Python\", year = 2014) {\n  plot <- ggplot(sentiments, aes(ave_sentiment, sr)) + \n    geom_boxplot()\n  ggsave(filename = paste0(\"sentiments-\", sr, \"-\", year, \".png\"), plot, path = \"plots/\")\n}\n#sr <- \"Python\"\n#year <- 2014\n#python_2014_sentiments <- conduct_sentiment_analysis()\n#plot_sentiments(python_2014_sentiments)"
  },
  {
    "objectID": "sentiment-analysis.html#executing-the-analysis",
    "href": "sentiment-analysis.html#executing-the-analysis",
    "title": "2  Sentiment Analysis",
    "section": "2.1 Executing the analysis",
    "text": "2.1 Executing the analysis\n\n\nCode\nsubs_of_interest <- c(\"learnpython\",\n                      \"Python\",\n                      \"learnprogramming\",\n                      \"programming\",\n                      \"AskProgramming\",\n                      \"CSEducation\",\n                      \"computerscience\",\n                      \"AskComputerScience\",\n                      \"compsci\",\n                      \"AskCompSci\",\n                      \"cscareerquestions\",\n                      \"coding\",\n                      \"javascript\",\n                      \"learnjavascript\",\n                      \"java\",\n                      \"learnjava\",\n                      \"haskell\",\n                      \"haskellquestions\",\n                      \"C_Programming\")\nyears <- c(2014, 2015, 2016, 2017)\n\nfor (sub in subs_of_interest) {\n  for (y in years) {\n    if (!file.exists(paste0(\"data/sentiments-\", sub, \"-\", y, \".rds\"))) {\n      tryCatch({\n        withTimeout({\n          sentiments <- conduct_sentiment_analysis(sr = sub, year = y)\n          plot_sentiments(sentiments, sr = sub, year = y)\n          write_rds(sentiments, file = paste0(\"data/sentiments-\", sub, \"-\", y, \".rds\"))\n        }, timeout = 600)\n      }, TimeoutException = function(ex) {\n        message(\"Processing \", sub, \" \", y, \" timed out :(\")\n      })\n    } else { message(\"Already processed \", sub, \" \", y)}\n  }\n}"
  },
  {
    "objectID": "sentiment-analysis.html#display-some-plots",
    "href": "sentiment-analysis.html#display-some-plots",
    "title": "2  Sentiment Analysis",
    "section": "2.2 Display some plots",
    "text": "2.2 Display some plots\n\n[r/Python r/learnPython][r/java r/learnjava][r/javascript r/learnjavascript][r/programming r/learnprogramming r/AskProgramming][r/computerscience r/AskComputerScience][r/compsci r/AskCompSci][r/programming r/coding]\n\n\n\n\nCode\nlibrary(ggpubr)\n\nfor (year in 2014:2017) {\n  assign(paste0(\"sentiments-python-\", year), \n         readRDS(paste0(\"data/sentiments-Python-\", year, \".rds\")))\n  assign(paste0(\"sentiments-learnpython-\", year), \n         readRDS(paste0(\"data/sentiments-learnpython-\", year, \".rds\")))\n}\n\n`sentiments-learnpython-2014` %<>% select(ave_sentiment) %>% mutate(year = 2014)\n`sentiments-learnpython-2015` %<>% select(ave_sentiment) %>% mutate(year = 2015)\n`sentiments-learnpython-2016` %<>% select(ave_sentiment) %>% mutate(year = 2016)\n`sentiments-learnpython-2017` %<>% select(ave_sentiment) %>% mutate(year = 2017)\n\n`sentiments-python-2014` %<>% select(ave_sentiment) %>% mutate(year = 2014)\n`sentiments-python-2015` %<>% select(ave_sentiment) %>% mutate(year = 2015)\n`sentiments-python-2016` %<>% select(ave_sentiment) %>% mutate(year = 2016)\n`sentiments-python-2017` %<>% select(ave_sentiment) %>% mutate(year = 2017)\n\np_sentiments <- rbind(`sentiments-python-2014`, \n      `sentiments-python-2015`, \n      `sentiments-python-2016`, \n      `sentiments-python-2017`) \np_sentiments %>% ggboxplot(x = \"year\", y = \"ave_sentiment\")\n\n\n\n\n\nCode\np_sentiments %>% ggviolin(x = \"year\", y = \"ave_sentiment\")\n\n\n\n\n\nCode\np_sentiments %>% group_by(year) %>% summarise(avg = mean(ave_sentiment))\n\n\n# A tibble: 4 × 2\n   year   avg\n  <dbl> <dbl>\n1  2014 0.184\n2  2015 0.151\n3  2016 0.110\n4  2017 0.108\n\n\nCode\none.way <- aov(year ~ ave_sentiment, data = p_sentiments)\nsummary(one.way)\n\n\n                 Df Sum Sq Mean Sq F value Pr(>F)    \nave_sentiment     1    261  261.18   232.7 <2e-16 ***\nResiduals     21701  24361    1.12                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nlp_sentiments <- rbind(`sentiments-learnpython-2014`, \n      `sentiments-learnpython-2015`, \n      `sentiments-learnpython-2016`, \n      `sentiments-learnpython-2017`) \nlp_sentiments %>% ggboxplot(x = \"year\", y = \"ave_sentiment\")\n\n\n\n\n\nCode\nlp_sentiments %>% ggviolin(x = \"year\", y = \"ave_sentiment\")\n\n\n\n\n\nCode\nlp_sentiments %>% group_by(year) %>% summarise(avg = mean(ave_sentiment))\n\n\n# A tibble: 4 × 2\n   year    avg\n  <dbl>  <dbl>\n1  2014 0.117 \n2  2015 0.102 \n3  2016 0.0917\n4  2017 0.0919\n\n\nCode\none.way <- aov(year ~ ave_sentiment, data = lp_sentiments)\nsummary(one.way)\n\n\n                 Df Sum Sq Mean Sq F value   Pr(>F)    \nave_sentiment     1     62   61.94   58.22 2.38e-14 ***\nResiduals     62421  66406    1.06                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "topic-modeling.html",
    "href": "topic-modeling.html",
    "title": "3  Topic Modeling",
    "section": "",
    "text": "Conduct topic modelingPlot Topics\n\n\n\n\nCode\nconduct_lda <- function(sr = \"Python\", year = 2014) {\n  \n  input_directory <- paste0(\"/scratch1/tsranso/reddit/subreddit-posts/\", sr, \"/\", year)\n\n  message(\"Opening input directory: \", input_directory)\n  posts <- spark_read_json(sc, path = input_directory, memory = FALSE)\n  message(\"Running LDA...\")\n  lda_betas <- posts %>% ml_lda(~ selftext, \n                      k = 6, \n                      max_iter = 1, \n                      min_token_length = 4,\n                      stop_words = sparklyr::ml_default_stop_words(sc), \n                      min_df = 5) %>%\n    tidy()\n  message(\"Computed LDA\")\n  return(lda_betas)\n}\n\n\n\n\n\n\nCode\nplot_betas <- function(betas, sr = \"Python\", year = \"2014\") {\n  if (nrow(lda_betas) == 0) { return() }\n  \n  plot <- betas %>% group_by(topic) %>%\n    top_n(15, beta) %>%\n    ungroup() %>%\n    arrange(topic, -beta) %>%\n    mutate(term = reorder(term, beta)) %>%\n    ggplot(aes(term, beta, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\") +\n    coord_flip()\n  ggsave(filename = paste0(sr, \"-\", year, \".png\"), plot, path = \"plots/\")\n}"
  },
  {
    "objectID": "topic-modeling.html#execute-the-topic-modeling",
    "href": "topic-modeling.html#execute-the-topic-modeling",
    "title": "3  Topic Modeling",
    "section": "3.1 Execute the topic modeling",
    "text": "3.1 Execute the topic modeling\n\n\nCode\nfor (sub in subs_of_interest) {\n  for (y in years) {\n    if (!file.exists(paste0(\"data/lda-\", sub, \"-\", y, \".rds\"))) {\n      lda_betas <- conduct_lda(sr = sub, year = y)\n      plot_betas(betas = lda_betas, sr = sub, year = y)\n      write_rds(lda_betas, file = paste0(\"data/lda-\", sub, \"-\", y, \".rds\"))\n    } else { message(\"Already processed \", sub, \" \", y)}\n  }\n}"
  },
  {
    "objectID": "topic-modeling.html#topics",
    "href": "topic-modeling.html#topics",
    "title": "3  Topic Modeling",
    "section": "3.2 Topics",
    "text": "3.2 Topics\n\n3.2.1 r/python\n\n2014201520162017\n\n\n\n\nCode\nknitr::include_graphics(\"plots/Python-2014.png\")\n\n\n\n\n\n\n\n\n\nCode\nknitr::include_graphics(\"plots/Python-2015.png\")\n\n\n\n\n\n\n\n\n\nCode\nknitr::include_graphics(\"plots/Python-2016.png\")\n\n\n\n\n\n\n\n\n\nCode\nknitr::include_graphics(\"plots/Python-2017.png\")\n\n\n\n\n\n\n\n\n\n\n3.2.2 r/learnpython\n\n2014201520162017\n\n\n\n\nCode\nknitr::include_graphics(\"plots/learnpython-2014.png\")\n\n\n\n\n\n\n\n\n\nCode\nknitr::include_graphics(\"plots/learnpython-2015.png\")\n\n\n\n\n\n\n\n\n\nCode\nknitr::include_graphics(\"plots/learnpython-2016.png\")\n\n\n\n\n\n\n\n\n\nCode\nknitr::include_graphics(\"plots/learnpython-2017.png\")"
  }
]