# Quantitative Ethnography

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pak)
library(sparklyr)
library(tidyverse)
library(magrittr)
library(jsonlite)
```

## Load Reddit Post Data

```{r set up spark}
Sys.setenv(SPARK_HOME="/software/external/spark/3.3.1")
Sys.setenv(JAVA_HOME="/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/openjdk-11.0.15_10-xo4fjahmlsjch52sftpoxby6kwbdfoib")
library(sparklyr)

options(sparklyr.log.console = TRUE)
config <- spark_config()
config["sparklyr.shell.driver-memory"] <- "10g"
config["spark.executor.memory"] <- "15G" # typically 4g

spark_master_info <- Sys.getenv("RSESSION_LOG_FILE") %>% 
  dirname %>% 
  paste0("/spark_master.info") %>% 
  readLines(n = 1)

# master node needs to be modified with each new job
sc <- spark_connect(master = spark_master_info, config = config)

```


```{r read in posts}
read_posts_that_contain <- function(sc, pattern) {
    for (sub in list.files("/scratch1/tsranso/reddit/subreddit-posts", full.names = TRUE)) {
    for (input_directory in list.files(sub, full.names = TRUE)) {
      sr <- basename(dirname(input_directory))
      #input_directory <- "/scratch1/tsranso/reddit/subreddit-posts/learnprogramming/2014"
      
      message("Opening input directory: ", input_directory)
      
      posts <- spark_read_json(sc, path = input_directory, memory = FALSE) %>% 
        mutate(selftext = str_to_lower(selftext), media = NULL, media_embed = NULL) %>% 
        filter(selftext %like% "%programmer%") %>% collect()
      write_rds(posts, paste0("data/", sr, ".programmer-posts.rds"))
    }
  }
  
  programmer_posts <- NULL
  
  for (file in list.files(path = "data", pattern = "*programmer-posts.rds", full.names = TRUE)) {
    new_posts <- read_rds(file) %>% 
      mutate(secure_media = NULL, secure_media_embed = NULL, created_utc = as.character(created_utc))
    programmer_posts <- bind_rows(programmer_posts, new_posts)
  }
  write_rds(programmer_posts, "data/programmer-posts.rds")
}

read_programmer_posts <- function(sc) {
  read_posts_that_contain(sc, "programmer")
}

if (!file.exists("data/programmer-posts.rds")) {
  read_programmer_posts(sc)
}

programmer_posts <- readRDS("data/programmer-posts.rds")
```

## A few descriptive stats about our data

```{r}
library(ggpubr)
library(ggiraph)
programmer_posts %>% group_by(domain) %>% summarize(count = n()) %>% arrange(-count)
programmer_posts %>% group_by(domain) %>% summarize(count = n()) %>% arrange(-count) %>% ggbarplot(x = "domain", y = "count") + rotate_x_text(45)
```

## A quick LDA

```{r}
#library(topicmodels)
```


## Segment and format data

```{r, eval=FALSE}
#programmer_posts %>% mutate(selftext = )
```


## Use ncoder to begin the semiautomated coding

```{r, eval=FALSE}
library(ncodeR)

# Load some data
data(RS.data)
rs = RS.data

###
# Create the Data code
###
code.interest = create.code(name = "Interest", expressions = c("computer","programming", "goals"), excerpts = rs$text)

# Handcode 30 excerpts for Data code
code.interest = handcode(code = code.data, n=5)

# Run test to see rho/kappa of current test set
code.interest = test(code = code.data, kappaThreshold = 0.65)

# View the summary, with the calcuated statistics
summary(code.data)

# Create the People code
code.recognition = create.code(name = "Recognition", expressions = c("peers","friends", "teachers", "family", "people"), excerpts = rs$text)

# Handcode 30 excerpts for People code
code.recognition = handcode(code = code.people, n=5)

# Run test
code.recognition = test(code = code.people, kappaThreshold = 0.65)

summary(code.recognition)

# Create the People code
code.people = create.code(name = "Competence", expressions = c("works", "programmed", "broke", "fixed", "wondering"), excerpts = rs$text)

# Handcode 30 excerpts for People code
code.people = handcode(code = code.people, n=5)

# Run test
code.people = test(code = code.people, kappaThreshold = 0.65)

summary(code.people)


###
# Generate a CodeSet for all Codes
###
code.set = code.set("Demo RS CodeSet", "CodeSet made for the demo", codes = c(code.data, code.people))

# Autocode the full set of excerpts, returning a data.frame
allcoded = autocode(x = code.set)

# Autocode, returning the Code.Set with codes containing updated $computerSets
allcoded = autocode(x = code.set, simplify = F)

# Convert the CodeSet directly to a data.frame using each Codes $computerSet
allcoded.data = as.data.frame(allcoded)
```


## check reliability with rhor

## exit out of spark session

```{r}
spark_disconnect(sc)
```

