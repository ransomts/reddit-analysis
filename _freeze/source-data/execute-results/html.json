{
  "hash": "695c5edc9cb6a85eb0c1aec2674d354d",
  "result": {
    "markdown": "# Source Reddit Data\n\n\n\n\n\n## Downloaded Data Stats\n\nI've pulled quite a bit of Reddit data to process. Files that begin with RS are subreddit data including post titles, selftext, user, etc. Files that begin with RC are comment data.\n\nLets take a look at how big some of this stuff is:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nprint(\"Years of posts data: \")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Years of posts data: \"\n```\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nfs::dir_ls(\"/scratch1/tsranso/reddit/posts/\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n/scratch1/tsranso/reddit/posts/2014 /scratch1/tsranso/reddit/posts/2015 \n/scratch1/tsranso/reddit/posts/2016 /scratch1/tsranso/reddit/posts/2017 \n```\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nprint(\"Size of one month of data: \")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Size of one month of data: \"\n```\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nfs::file_size(\"/scratch1/tsranso/reddit/posts/2017/RS_2017-01.json\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n16.1G\n```\n:::\n:::\n\n\n## Spark\n\nI'm using the Palmetto cluster to make quick work of processing through our Reddit data.\n\n### Set up and connect to spark\n\n\n::: {.cell hash='cache/spark setup_9bb1e33310259452edd712d20c7cd4d5'}\n\n```{.r .cell-code}\nSys.setenv(SPARK_HOME = \"/software/external/spark/3.3.1\")\nSys.setenv(JAVA_HOME = \"/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/openjdk-11.0.15_10-xo4fjahmlsjch52sftpoxby6kwbdfoib\")\nlibrary(sparklyr)\n\noptions(sparklyr.log.console = TRUE)\nconfig <- spark_config()\nconfig[\"sparklyr.shell.driver-memory\"] <- \"10g\"\nconfig[\"spark.executor.memory\"] <- \"15G\"  # typically 4g\n\nspark_master_info <- Sys.getenv(\"RSESSION_LOG_FILE\") %>%\n    dirname %>%\n    paste0(\"/spark_master.info\") %>%\n    readLines(n = 1)\n\n# master node needs to be modified with each new job\nsc <- spark_connect(master = spark_master_info, config = config)\n```\n:::\n\n\n### Data filtering functions\n\nWe need a few functions to read through the Reddit data, for example here's something we can use to get out the text posts for a given subreddit.\n\n\n::: {.cell hash='cache/pull posts function def_dbc974abcade331a3ca16ac8be3db795'}\n\n```{.r .cell-code}\npull_posts_of_subreddit <- function(years = c(2014, 2015, 2016, 2017), sr = \"Python\",\n    write_to_dir = FALSE) {\n    for (year in years) {\n        year_dir <- paste0(\"/scratch1/tsranso/reddit/posts/\", year)\n        message(\"Reading from directory: \", year_dir)\n        message(\"file exists: \", file.exists(year_dir))\n        posts <- spark_read_json(sc, path = year_dir, memory = FALSE) %>%\n            filter(subreddit %in% c(sr)) %>%\n            mutate(selftext = regexp_replace(selftext, \"\\\\n|&nbsp;|<[^>]*>|[^A-Za-z|']\",\n                \" \")) %>%\n            mutate(selftext = str_trim(selftext)) %>%\n            filter(!selftext %in% c(\"\", \"deleted\", \"title\", \"removed\"))\n\n        if (write_to_dir) {\n            output_directory <- paste0(\"/scratch1/tsranso/reddit/subreddit-posts/\",\n                sr, \"/\", year)\n            message(\"Writing filtered post data to: \", output_directory)\n            spark_write_json(posts, path = output_directory, mode = \"overwrite\")\n        }\n    }\n\n    return(posts)\n}\npull_posts_of_subreddit(write_to_dir = TRUE)\n```\n:::\n\n\nAnd another one to get all the comments from a subreddit:\n\n\n::: {.cell hash='cache/pull comments function def_8471a34716393af04dd61a52b0e99a84'}\n\n```{.r .cell-code}\npull_comments_of_subreddit <- function(years = c(2014, 2014, 2016, 2017), sr = \"python\",\n    write_to_dir = FALSE) {\n    for (year in years) {\n        year_dir <- paste0(\"/scratch1/tsranso/reddit/comments/\", year)\n        comments <- spark_read_json(sc, path = year_dir, memory = FALSE) %>%\n            filter(subreddit %in% c(sr) & selftext != \"\") %>%\n            mutate(selftext = regexp_replace(selftext, \"\\\\n|&nbsp;|<[^>]*>|[^A-Za-z|']\",\n                \" \")) %>%\n            mutate(selftext = str_trim(selftext)) %>%\n            filter(!selftext %in% c(\"\", \"deleted\", \"title\", \"removed\"))\n\n        if (write_to_dir) {\n            output_directory <- paste0(\"/scratch1/tsranso/reddit/subreddit-comments/\",\n                sr, \"/\", year)\n            message(\"Writing filtered comment data to: \", output_directory)\n            spark_write_json(comments, path = output_directory, mode = \"overwrite\")\n        }\n    }\n\n    return(posts)\n}\n```\n:::\n\n\n## Subreddits of interest\n\nThere are lots of great (and many not so great) communities on Reddit. Here is the list of subreddits that are considered in this project:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubs_of_interest <- c(\"learnpython\", \"Python\", \"learnprogramming\", \"programming\",\n    \"AskProgramming\", \"CSEducation\", \"computerscience\", \"AskComputerScience\", \"compsci\",\n    \"AskCompSci\", \"cscareerquestions\", \"coding\", \"javascript\", \"learnjavascript\",\n    \"java\", \"learnjava\", \"haskell\", \"haskellquestions\", \"C_Programming\")\n# knitr::kable(subs_of_interest, caption = 'Subreddits of interest')\nsubs_of_interest %>%\n    kable() %>%\n    kable_styling(\"striped\") %>%\n    scroll_box(height = \"200px\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:200px; \"><table class=\"table table-striped\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\"> x </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> learnpython </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Python </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> learnprogramming </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> programming </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> AskProgramming </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> CSEducation </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> computerscience </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> AskComputerScience </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> compsci </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> AskCompSci </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> cscareerquestions </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> coding </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> javascript </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> learnjavascript </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> java </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> learnjava </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> haskell </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> haskellquestions </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> C_Programming </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\n\n::: {.panel-tabset}\n\n### Pull Posts\n\nLet's loop through the subreddits of interest and pull out the text posts! Note the write_to_dir option here that caches the pulled posts into smaller files so we don't have to do this repeatedly.\n\n\n::: {.cell hash='cache/pull posts_1cec00a5f79d628afe3f88641080c2f6'}\n\n```{.r .cell-code  code-fold=\"show\"}\nfor (sub in subs_of_interest) {\n    pull_posts_of_subreddit(sr = sub, write_to_dir = TRUE)\n}\n```\n:::\n\n\n### Pull Comments\n\n\n::: {.cell hash='cache/pull comments_ebdac7e5225b8735f35567b7544611c2'}\n\n```{.r .cell-code  code-fold=\"show\"}\nfor (sub in subs_of_interest) {\n    pull_comments_of_subreddit(sr = sub, write_to_dir = TRUE)\n}\n```\n:::\n\n\n:::\n\n# Data Structure\n\n\n::: {.cell hash='cache/unnamed-chunk-2_800307f9c4441c647b5553b1cd8a2dbd'}\n\n```{.r .cell-code}\nsr <- \"Python\"\nyear <- 2017\n```\n:::\n\n\n\n## Post Data\n\n\n::: {.cell hash='cache/unnamed-chunk-3_02cd6c6e519fb5ff9c92d662e380a804'}\n\n```{.r .cell-code}\ninput_directory <- paste0(\"/scratch1/tsranso/reddit/subreddit-posts/\", sr, \"/\", year)\nposts <- spark_read_json(sc, path = input_directory, memory = FALSE)\nposts %>%\n    head %>%\n    print\n```\n:::\n\n\n## Comment Data\n\n\n::: {.cell hash='cache/unnamed-chunk-4_e4a8299cea3e82175747b65b06de4fa4'}\n\n```{.r .cell-code}\ninput_directory <- paste0(\"/scratch1/tsranso/reddit/subreddit-comments/\", sr, \"/\",\n    year)\ncomments <- spark_read_json(sc, path = input_directory, memory = FALSE)\ncomments %>%\n    head %>%\n    print\n```\n:::\n\n\n# Close spark connection\n\n\n::: {.cell hash='cache/unnamed-chunk-5_89906b19bde63daf89da9e1d1bf58a73'}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}